<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Matteo Vilucchio">
  <title>Publications - Matteo Vilucchio</title>
  <!-- Apply theme immediately to prevent flash of wrong theme -->
  <script src="/assets/js/util.js"></script>
  <script>
    // Apply theme immediately to prevent flash
    (function() {
      const savedTheme = localStorage.getItem('theme');
      if (savedTheme === 'dark') {
        document.documentElement.setAttribute('data-theme', 'dark');
      } else if (savedTheme === 'light') {
        document.documentElement.setAttribute('data-theme', 'light');
      } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.documentElement.setAttribute('data-theme', 'dark');
      }
    })();
  </script>
  <link rel="stylesheet" href="/assets/css/main.css?v=1.0.0">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Publications | Matteo Vilucchio</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Publications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="PhD Student in Machine Learning and Statistical Physics at EFPL." />
<meta property="og:description" content="PhD Student in Machine Learning and Statistical Physics at EFPL." />
<link rel="canonical" href="http://localhost:4000/publications/" />
<meta property="og:url" content="http://localhost:4000/publications/" />
<meta property="og:site_name" content="Matteo Vilucchio" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Publications" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"PhD Student in Machine Learning and Statistical Physics at EFPL.","headline":"Publications","url":"http://localhost:4000/publications/"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body class="">
  <header>
    <nav>
      <h1>
        
          <a href="/">
            <span class="firstname">Matteo</span> <span class="lastname">Vilucchio</span>
          </a>
        
      </h1>
      <ul>
        <li><a href="/">Home</a></li>
        <li><a href="/publications/">Publications</a></li>
        <li><a href="/cv/">CV</a></li>
        <li><a href="/other/">Other</a></li>
        <li>
          <button id="theme-toggle" class="theme-toggle" aria-label="Toggle dark mode">
            <svg id="light-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
              <circle cx="12" cy="12" r="5"></circle>
              <line x1="12" y1="1" x2="12" y2="3"></line>
              <line x1="12" y1="21" x2="12" y2="23"></line>
              <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
              <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
              <line x1="1" y1="12" x2="3" y2="12"></line>
              <line x1="21" y1="12" x2="23" y2="12"></line>
              <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
              <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
            </svg>
            <svg id="dark-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
              <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
            </svg>
          </button>
        </li>
      </ul>
    </nav>
  </header>

  <main class="site-content">
    <h1 id="publications-and-research">Publications and Research</h1>

<div class="section-tabs">
  <button class="section-tab active" data-section="publications-section">Publications</button>
  <button class="section-tab" data-section="preprints-section">Preprints</button>
</div>

<section id="publications-section" class="publication-section active">
  <h2 class="section-title">Publications</h2>
  
  
  
    <div class="year-section">
      <h3 class="year">2025</h3>
      
      
        
          <div class="publication">
            <div class="publication-title">
              A High Dimensional Statistical Model for Adversarial Training: Geometry and Trade-offs
            </div>
            
            <div class="publication-authors">
              Kasimir Tanner, Matteo Vilucchio, Bruno Loureiro, Florent Krzakala
            </div>
            
            <div class="publication-venue">
              <em>The 28th International Conference on Artificial Intelligence and Statistics</em>, 2025
            </div>
            
            <div class="publication-links">
              
                <a class="abstract-toggle" data-pub-id="pub-1">Abstract</a>
                <div class="abstract-content" id="abstract-pub-1" style="display: none;">
                  This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension d and the number of data points n diverge with a fixed ratio. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses for a Block Feature Model. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. We show that the the presence of multiple different feature types is crucial to the high sample complexity performances of adversarial training. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust features during training, identifying a uniform protection as an inherently effective defence mechanism.
                </div>
              
              
              
              
              
              
              
              
              
                <a href="https://github.com/IdePHICS/Adversarial-Setting" target="_blank">Code</a>
              
              
              
            </div>
          </div>
        
      
        
      
    </div>
  
    <div class="year-section">
      <h3 class="year">2024</h3>
      
      
        
      
        
          <div class="publication">
            <div class="publication-title">
              Asymptotic Characterisation of the Performance of Robust Linear Regression in the Presence of Outliers
            </div>
            
            <div class="publication-authors">
              Matteo Vilucchio, Emanuele Troiani, Vittorio Erba, Florent Krzakala
            </div>
            
            <div class="publication-venue">
              <em>The 27th International Conference on Artificial Intelligence and Statistics</em>, 2024
            </div>
            
            <div class="publication-links">
              
                <a class="abstract-toggle" data-pub-id="pub-2">Abstract</a>
                <div class="abstract-content" id="abstract-pub-2" style="display: none;">
                  We study robust linear regression in high-dimension, when both the dimension and the number of data points diverge with a fixed ratio, and study a data model that includes outliers. We provide exact asymptotics for the performances of the empirical risk minimisation (ERM) using L2-regularised L2, L1, and Huber losses, which are the standard approach to such problems. We focus on two metrics for the performance: the generalisation error to similar datasets with outliers, and the estimation error of the original, unpolluted function. Our results are compared with the information theoretic Bayes-optimal estimation bound. For the generalization error, we find that optimally-regularised ERM is asymptotically consistent in the large sample complexity limit if one perform a simple calibration, and compute the rates of convergence. For the estimation error however, we show that due to a norm calibration mismatch, the consistency of the estimator requires an oracle estimate of the optimal norm, or the presence of a cross-validation set not corrupted by the outliers. We examine in detail how performance depends on the loss function and on the degree of outlier corruption in the training set and identify a region of parameters where the optimal performance of the Huber loss is identical to that of the L2 loss, offering insights into the use cases of different loss functions.
                </div>
              
              
              
                <a class="bibtex-toggle" data-pub-id="pub-2">BibTeX</a>
                <div class="bibtex-content" id="bibtex-pub-2" style="display: none;">
                  <pre><code>@inproceedings{vilucchio2024asymptotic, title={Asymptotic characterisation of the performance of robust linear regression in the presence of outliers}, author={Vilucchio, Matteo and Troiani, Emanuele and Erba, Vittorio and Krzakala, Florent}, booktitle={International Conference on Artificial Intelligence and Statistics}, pages={811--819}, year={2024}, organization={PMLR} }</code></pre>
                  <button class="copy-bibtex" data-bibtex-id="pub-2">Copy to clipboard</button>
                </div>
              
              
              
              
              
                <a href="https://proceedings.mlr.press/v238/vilucchio24a/vilucchio24a.pdf" target="_blank" class="pdf-link">PDF</a>
              
              
              
                <a href="https://github.com/IdePHICS/RobustRegression" target="_blank">Code</a>
              
              
              
            </div>
          </div>
        
      
    </div>
  
</section>

<section id="preprints-section" class="publication-section">
  <h2 class="section-title">Preprints</h2>
  
  
  
    <div class="year-section">
      <h3 class="year">2025</h3>
      
      
        
          <div class="publication">
            <div class="publication-title">
              Asymptotics of Non-Convex Generalized Linear Models in High-Dimensions: A proof of the replica formula
            </div>
            
            <div class="publication-authors">
              Matteo Vilucchio, Yatin Dandi, Cedric Gerbelot, Florent Krzakala
            </div>
            
            <div class="publication-venue">
              <em>arXiv</em>, 2025
            </div>
            
            <div class="publication-links">
              
                <a class="abstract-toggle" data-pub-id="pre-1">Abstract</a>
                <div class="abstract-content" id="abstract-pre-1" style="display: none;">
                  The analytic characterization of the high-dimensional behavior of optimization for Generalized Linear Models (GLMs) with Gaussian data has been a central focus in statistics and probability in recent years. While convex cases, such as the LASSO, ridge regression, and logistic regression, have been extensively studied using a variety of techniques, the non-convex case remains far less understood despite its significance. A non-rigorous statistical physics framework has provided remarkable predictions for the behavior of high-dimensional optimization problems, but rigorously establishing their validity for non-convex problems has remained a fundamental challenge. In this work, we address this challenge by developing a systematic framework that rigorously proves replica-symmetric formulas for non-convex GLMs and precisely determines the conditions under which these formulas are valid. Remarkably, the rigorous replica-symmetric predictions align exactly with the conjectures made by physicists, and the so-called replicon condition. The originality of our approach lies in connecting two powerful theoretical tools: the Gaussian Min-Max Theorem, which we use to provide precise lower bounds, and Approximate Message Passing (AMP), which is shown to achieve these bounds algorithmically. We demonstrate the utility of this framework through significant applications: (i) by proving the optimality of the Tukey loss over the more commonly used Huber loss under a ε contaminated data model, (ii) establishing the optimality of negative regularization in high-dimensional non-convex regression and (iii) characterizing the performance limits of linearized AMP algorithms. By rigorously validating statistical physics predictions in non-convex settings, we aim to open new pathways for analyzing increasingly complex optimization landscapes beyond the convex regime.
                </div>
              
              
              
                <a class="bibtex-toggle" data-pub-id="pre-1">BibTeX</a>
                <div class="bibtex-content" id="bibtex-pre-1" style="display: none;">
                  <pre><code>@article{vilucchio2025asymptotics, title={Asymptotics of non-convex generalized linear models in high-dimensions: A proof of the replica formula}, author={Vilucchio, Matteo and Dandi, Yatin and Gerbelot, Cedric and Krzakala, Florent}, journal={arXiv preprint arXiv:2502.20003}, year={2025} }</code></pre>
                  <button class="copy-bibtex" data-bibtex-id="pre-1">Copy to clipboard</button>
                </div>
              
              
              
                <a href="https://arxiv.org/abs/2502.20003" target="_blank">arXiv</a>
              
              
              
              
              
                <a href="https://arxiv.org/pdf/2502.20003" target="_blank" class="pdf-link">PDF</a>
              
              
              
              
              
            </div>
          </div>
        
      
        
      
        
      
    </div>
  
    <div class="year-section">
      <h3 class="year">2024</h3>
      
      
        
      
        
          <div class="publication">
            <div class="publication-title">
              On the Geometry of Regularization in Adversarial Training: High-Dimensional Asymptotics and Generalization Bounds
            </div>
            
            <div class="publication-authors">
              Matteo Vilucchio, Nikolaos Tsilivis, Bruno Loureiro, Julia Kempe
            </div>
            
            <div class="publication-venue">
              <em>arXiv</em>, 2024
            </div>
            
            <div class="publication-links">
              
                <a class="abstract-toggle" data-pub-id="pre-2">Abstract</a>
                <div class="abstract-content" id="abstract-pre-2" style="display: none;">
                  Regularization, whether explicit in terms of a penalty in the loss or implicit in the choice of algorithm, is a cornerstone of modern machine learning. Indeed, controlling the complexity of the model class is particularly important when data is scarce, noisy or contaminated, as it translates a statistical belief on the underlying structure of the data. This work investigates the question of how to choose the regularization norm in the context of high-dimensional adversarial training for binary classification. To this end, we first derive an exact asymptotic description of the robust, regularized empirical risk minimizer for various types of adversarial attacks and regularization norms (including non-Lp norms). We complement this analysis with a uniform convergence analysis, deriving bounds on the Rademacher Complexity for this class of problems. Leveraging our theoretical results, we quantitatively characterize the relationship between perturbation size and the optimal choice of norm, confirming the intuition that, in the data scarce regime, the type of regularization becomes increasingly important for adversarial training as perturbations grow in size.
                </div>
              
              
              
                <a class="bibtex-toggle" data-pub-id="pre-2">BibTeX</a>
                <div class="bibtex-content" id="bibtex-pre-2" style="display: none;">
                  <pre><code>@article{vilucchio2024geometry, title={On the Geometry of Regularization in Adversarial Training: High-Dimensional Asymptotics and Generalization Bounds}, author={Vilucchio, Matteo and Tsilivis, Nikolaos and Loureiro, Bruno and Kempe, Julia}, journal={arXiv preprint arXiv:2410.16073}, year={2024} }</code></pre>
                  <button class="copy-bibtex" data-bibtex-id="pre-2">Copy to clipboard</button>
                </div>
              
              
              
                <a href="https://arxiv.org/abs/2410.16073" target="_blank">arXiv</a>
              
              
              
              
              
                <a href="https://arxiv.org/pdf/2410.16073" target="_blank" class="pdf-link">PDF</a>
              
              
              
              
              
            </div>
          </div>
        
      
        
      
    </div>
  
    <div class="year-section">
      <h3 class="year">2021</h3>
      
      
        
      
        
      
        
          <div class="publication">
            <div class="publication-title">
              Genealogical Population-Based Training for Hyperparameter Optimization
            </div>
            
            <div class="publication-authors">
              Antoine Scardigli, Paul Fournier, Matteo Vilucchio, David Naccache
            </div>
            
            <div class="publication-venue">
              <em>arXiv</em>, 2021
            </div>
            
            <div class="publication-links">
              
                <a class="abstract-toggle" data-pub-id="pre-3">Abstract</a>
                <div class="abstract-content" id="abstract-pre-3" style="display: none;">
                  Regularization, whether explicit in terms of a penalty in the loss or implicit in the choice of algorithm, is a cornerstone of modern machine learning. Indeed, controlling the complexity of the model class is particularly important when data is scarce, noisy or contaminated, as it translates a statistical belief on the underlying structure of the data. This work investigates the question of how to choose the regularization norm in the context of high-dimensional adversarial training for binary classification. To this end, we first derive an exact asymptotic description of the robust, regularized empirical risk minimizer for various types of adversarial attacks and regularization norms (including non-Lp norms). We complement this analysis with a uniform convergence analysis, deriving bounds on the Rademacher Complexity for this class of problems. Leveraging our theoretical results, we quantitatively characterize the relationship between perturbation size and the optimal choice of norm, confirming the intuition that, in the data scarce regime, the type of regularization becomes increasingly important for adversarial training as perturbations grow in size.
                </div>
              
              
              
                <a class="bibtex-toggle" data-pub-id="pre-3">BibTeX</a>
                <div class="bibtex-content" id="bibtex-pre-3" style="display: none;">
                  <pre><code>@article{scardigli2021genealogical, title={Genealogical Population-Based Training for Hyperparameter Optimization}, author={Scardigli, Antoine and Fournier, Paul and Vilucchio, Matteo and Naccache, David}, journal={arXiv preprint arXiv:2109.14925}, year={2021} }</code></pre>
                  <button class="copy-bibtex" data-bibtex-id="pre-3">Copy to clipboard</button>
                </div>
              
              
              
                <a href="https://arxiv.org/abs/2109.14925" target="_blank">arXiv</a>
              
              
              
              
              
                <a href="https://arxiv.org/pdf/2109.14925" target="_blank" class="pdf-link">PDF</a>
              
              
              
              
              
            </div>
          </div>
        
      
    </div>
  
</section>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Section tabs functionality
    const sectionTabs = document.querySelectorAll('.section-tab');
    const sections = document.querySelectorAll('.publication-section');
    
    sectionTabs.forEach(tab => {
      tab.addEventListener('click', function() {
        // Remove active class from all tabs and sections
        sectionTabs.forEach(t => t.classList.remove('active'));
        sections.forEach(s => s.classList.remove('active'));
        
        // Add active class to clicked tab
        this.classList.add('active');
        
        // Show corresponding section
        const sectionId = this.getAttribute('data-section');
        document.getElementById(sectionId).classList.add('active');
      });
    });
    
    // Add click handlers for abstract toggles
    const abstractToggles = document.querySelectorAll('.abstract-toggle');
    abstractToggles.forEach(toggle => {
      toggle.addEventListener('click', function() {
        const pubId = this.getAttribute('data-pub-id');
        const abstract = document.getElementById('abstract-' + pubId);
        
        if (abstract.style.display === 'none') {
          abstract.style.display = 'block';
          this.textContent = 'Hide Abstract';
        } else {
          abstract.style.display = 'none';
          this.textContent = 'Abstract';
        }
      });
    });
    
    // Add click handlers for bibtex toggles
    const bibtexToggles = document.querySelectorAll('.bibtex-toggle');
    bibtexToggles.forEach(toggle => {
      toggle.addEventListener('click', function() {
        const pubId = this.getAttribute('data-pub-id');
        const bibtex = document.getElementById('bibtex-' + pubId);
        
        if (bibtex.style.display === 'none') {
          bibtex.style.display = 'block';
          this.textContent = 'Hide BibTeX';
        } else {
          bibtex.style.display = 'none';
          this.textContent = 'BibTeX';
        }
      });
    });
    
    // Add click handlers for copy buttons
    const copyButtons = document.querySelectorAll('.copy-bibtex');
    copyButtons.forEach(button => {
      button.addEventListener('click', function() {
        const bibtexId = this.getAttribute('data-bibtex-id');
        const bibtexContent = document.querySelector(`#bibtex-${bibtexId} pre code`).textContent;
        
        navigator.clipboard.writeText(bibtexContent).then(() => {
          // Show copied feedback
          const originalText = this.textContent;
          this.textContent = 'Copied!';
          
          // Reset button text after 2 seconds
          setTimeout(() => {
            this.textContent = originalText;
          }, 2000);
        }).catch(err => {
          console.error('Failed to copy: ', err);
        });
      });
    });
  });
</script>


  </main>

  <footer>
    <div class="footer-content">
      <p>&copy; 2025 Matteo Vilucchio. Almost entirely vibe coded.</p>
    </div>
  </footer>

  <!-- Main JavaScript with all functionality -->
  <script src="/assets/js/main.js?v=1.0.0"></script>
</body>
</html>